w <- solve(t(X)%*%X)%*%t(X)%*%y  # calculating weights that minimize E_in (squared error)
y_model <- sign(as.vector(X%*%w))      # apply the hypothesis function to the data
e_in <- sum(y != y_model)/N_train      # calculate in-sample error
e_in[i] <- e_in   # store the in-sample error
## Function for generating the data and, if specified, a target function
data.generate <- function(n = 10, ext = 1, generateTarget = FALSE){
# Generate the points
x1 <- runif(n, -ext, ext)
x2 <- runif(n, -ext, ext)
if (!generateTarget)
return(data.frame(x1, x2))
# Draw a random line in the area (target function)
point <- runif(2, -ext, ext)
point2 <- runif(2, -ext, ext)
slope <- (point2[2] - point[2]) / (point2[1] - point[1])
intercept <- point[2] - slope * point[1]
# Set up a factor for point classification
y <- as.numeric(x1 * slope + intercept > x2) * 2 - 1
# Return the values in a list
data <- data.frame(x1,x2,y)
return(list(data = data,slope = slope, intercept = intercept))
}
simulate.PLA <- function(numTrials = 1000, maxIterations = Inf, simulation = data.generate) {
iterations <- numeric(0)    # initializing the iteration and misclassification probability vectors
probability <- numeric(0)
# Running the algorithm through numTrials trials
#numTrials <- 1000
for (i in 1:numTrials){
generated  <-  simulation(n=100, generateTarget = TRUE) # generating points (set n=10 or n=100) and target function
input  <-  as.matrix(cbind(1, generated$data[c(1,2)])) # creating the input matrix
w  <-  c(0,0,0)  # initializing the weight vector
#res <- apply(input,1,function(x) t(w)%*%x)  # multiplying transpose of w with each row of input matrix to get initial hypothesis function
res <- as.vector(input %*% w) #equivalent operation
best <- list(sum(sign(res) != generated$data$y)/length(res), w)  # initializing list to keep track of the best in-sample error achieved so far and the weight vector that produced it
k <- 0  # initializing iteration counter
while (any(sign(res) != generated$data$y) && k < maxIterations) # as long as any of the elements of res have a sign mismatch and the iterations threshold has not been reached
# the true output y, the PLA algorithm continues to iterate
{
cat("Iteration:", k, "\n")
misclassified <- which(sign(res)!=generated$data$y)  # getting the indices of the points for which hypothesis is wrong
ifelse (length(misclassified)==1, n <- misclassified, n <- sample(misclassified,1))  # randomly choose one of these points
w <- w + generated$data$y[n]*input[n,]  # update the weights
res <- apply(input,1,function(x) t(w)%*%x)  # use new weights to update the hypothesis function
e_in <- sum(sign(res) != generated$data$y)/length(res) # calculate in-sample error
if (e_in < best[[1]]) {
best <- list(e_in, w) # if a the current weight vector is better than the previous best, store it
}
k <- k+1  # increment iteration count
}
w <- best[[2]] #selecting the best weight vector discovered by the algorithm
iterations[i] <- k  # store the number of iterations needed in this run
new.data <- simulation(10000)  #  generating the test points in order to examine out-of-sample performance
f  <-  as.numeric(new.data$x1 * generated$slope + generated$intercept > new.data$x2) * 2 - 1  # classifying points according to the true function f
g  <-  as.numeric(new.data$x1 * (-w[2]/w[3]) - w[1]/w[3] > new.data$x2) * 2 - 1  # classifying points according to the hypothesised function g, using the
# final weights provided by PLA
probability[i] <- sum(f != g)/10000  # store the misclassification error from this run
}
# Plot the points and f and g functions from the last iteration (purely illustrative purposes)
library(ggplot2)
qplot(x1,x2,col= as.factor(y), data = generated$data) + geom_abline(intercept = generated$intercept, slope = generated$slope)+
geom_abline(intercept = -w[1]/w[3], slope = -w[2]/w[3], col=3)
# Final results: average of iterations and estimated misclassification probabilities
list(iterations = mean(iterations), probability = mean(probability))
}
## Uses regression to approximate target functions on simulated data
## Returns the average in-sample and out-of-sample error across these hypotheses
## Plots
simulate.classificationByRegression <- function(N_train = 100, N_test = 1000, numTrials = 1000, PLA = FALSE, maxIterations = Inf) {
e_in <- numeric(0)  # initializing vector to hold in-sample error measures
e_out <- numeric(0) # initializing vector to hold out-of-sample error measures
iterations <- as.numeric(NA) # initializing vector to hold number of iterations
for (i in 1:numTrials) {
sim <- data.generate(N_train, generateTarget = TRUE)
X <- matrix(c(rep(1, N_train), sim$data$x1, sim$data$x2), ncol = 3, dimnames = list(c(), c('x0','x1', 'x2')))
y <- sim$data$y
w <- solve(t(X)%*%X)%*%t(X)%*%y  # calculating weights that minimize E_in (squared error)
y_model <- sign(as.vector(X%*%w))      # apply the hypothesis function to the data
e_train <- sum(y != y_model)/N_train      # calculate in-sample error
if(PLA) {
best <- list(e_train, w)  # initializing list to keep track of the best in-sample error achieved so far and the weight vector that produced it
k <- 0  # initializing iteration counter
while (any(sign(y_model) != y) && k < maxIterations) # as long as any of the elements of y_model do not match the true output, y, and the iterations threshold has not been reached
# the PLA algorithm continues to iterate
{
cat("Iteration:", k, "\n")
misclassified <- which(sign(y_model) != y)  # getting the indices of the points for which hypothesis is wrong
ifelse (length(misclassified) == 1, n <- misclassified, n <- sample(misclassified, 1))  # randomly choose one of these points
w <- w + y[n] * X[n,]  # update the weights
y_model <- apply(X, 1, function(x) t(w)%*%x)  # use new weights to update the hypothesis function
e_train <- sum(sign(y_model) != y)/length(y_model) # calculate in-sample error
if (e_train < best[[1]]) {
best <- list(e_in, w) # if a the current weight vector is better than the previous best, store it
}
k <- k+1  # increment iteration count
}
e_train <- best[[1]] # updating e_in
w <- best[[2]] # selecting the best weight vector discovered by the algorithm
iterations[i] <- k  # store the number of iterations needed in this run
}
e_in[i] <- e_train   # store the in-sample error
X <- as.matrix(cbind(1, data.generate(N_test)))  # generate test data set to examine out of sample performance
y <- as.numeric(X[, 'x1'] * sim$slope + sim$intercept > X[, 'x2']) * 2 - 1  # classify the test points using the target function generated with the training data set
y_model <- sign(as.vector(X%*%w))  # apply the hypothesis function to the test data
e_out[i] <- sum(y != y_model)/N_test  # calculate and store out-of-sample error
}
library(ggplot2)
library(gridExtra)
plot1 <- qplot(sim$data$x1, sim$data$x2, col= as.factor(sim$data$y), data = sim$data, xlab = 'x1', ylab = 'x2', main = 'Training Data') +
geom_abline(intercept = sim$intercept, slope = sim$slope) +
geom_abline(intercept = -w[1]/w[3], slope = -w[2]/w[3], col=3)
test <- data.frame(x1 = X[, 'x1'], x2 = X[, 'x2'], y = y)
plot2 <- qplot(test$x1, test$x2, col= as.factor(test$y), data = test, xlab = 'x1', ylab = 'x2', main = 'The Same Regression Model and Target Function Against Test Data') +
geom_abline(intercept = sim$intercept, slope = sim$slope) +
geom_abline(intercept = -w[1]/w[3], slope = -w[2]/w[3], col=3)
grid.arrange(plot1, plot2, ncol=2)
list(e_in = mean(e_in), e_out = mean(e_out), iterations = mean(iterations)) # return the averages of the error measures and the number of iterations run by the PLA, if applicable
}
simulate.classificationByRegression()
simulate.classificationByRegression(PLA = TRUE)
simulate.classificationByRegression(N_train = 10, PLA = TRUE)
warnings()
sim <- data.generate(N_train, generateTarget = TRUE)
X <- matrix(c(rep(1, N_train), sim$data$x1, sim$data$x2), ncol = 3, dimnames = list(c(), c('x0','x1', 'x2')))
y <- sim$data$y
w <- solve(t(X)%*%X)%*%t(X)%*%y  # calculating weights that minimize E_in (squared error)
y_model <- sign(as.vector(X%*%w))      # apply the hypothesis function to the data
e_train <- sum(y != y_model)/N_train      # calculate in-sample error
e_train
best <- list(e_train, w)  # initializing list to keep track of the best in-sample error achieved so far and the weight vector that produced it
k <- 0  # initializing iteration counter
best[[1]]
e_train
cat("Iteration:", k, "\n")
misclassified <- which(sign(y_model) != y)  # getting the indices of the points for which hypothesis is wrong
ifelse (length(misclassified) == 1, n <- misclassified, n <- sample(misclassified, 1))  # randomly choose one of these points
w <- w + y[n] * X[n,]  # update the weights
y_model <- apply(X, 1, function(x) t(w)%*%x)  # use new weights to update the hypothesis function
e_train <- sum(sign(y_model) != y)/length(y_model) # calculate in-sample error
e_train
e_train <- best[[1]] # updating e_in
e_train
simulate.classificationByRegression()
## Function for generating the data and, if specified, a target function
data.generate <- function(n = 10, ext = 1, generateTarget = FALSE){
# Generate the points
x1 <- runif(n, -ext, ext)
x2 <- runif(n, -ext, ext)
if (!generateTarget)
return(data.frame(x1, x2))
# Draw a random line in the area (target function)
point <- runif(2, -ext, ext)
point2 <- runif(2, -ext, ext)
slope <- (point2[2] - point[2]) / (point2[1] - point[1])
intercept <- point[2] - slope * point[1]
# Set up a factor for point classification
y <- as.numeric(x1 * slope + intercept > x2) * 2 - 1
# Return the values in a list
data <- data.frame(x1,x2,y)
return(list(data = data,slope = slope, intercept = intercept))
}
## Uses regression to approximate target functions on simulated data
## Returns the average in-sample and out-of-sample error across these hypotheses
## Plots
simulate.classificationByRegression <- function(N_train = 100, N_test = 1000, numTrials = 1000, PLA = FALSE, maxIterations = Inf) {
e_in <- numeric(0)  # initializing vector to hold in-sample error measures
e_out <- numeric(0) # initializing vector to hold out-of-sample error measures
iterations <- as.numeric(NA) # initializing vector to hold number of iterations
for (i in 1:numTrials) {
sim <- data.generate(N_train, generateTarget = TRUE)
X <- matrix(c(rep(1, N_train), sim$data$x1, sim$data$x2), ncol = 3, dimnames = list(c(), c('x0','x1', 'x2')))
y <- sim$data$y
w <- solve(t(X)%*%X)%*%t(X)%*%y  # calculating weights that minimize E_in (squared error)
y_model <- sign(as.vector(X%*%w))      # apply the hypothesis function to the data
e_train <- sum(y != y_model)/N_train      # calculate in-sample error
if(PLA) {
best <- list(e_train, w)  # initializing list to keep track of the best in-sample error achieved so far and the weight vector that produced it
k <- 0  # initializing iteration counter
while (any(sign(y_model) != y) && k < maxIterations) # as long as any of the elements of y_model do not match the true output, y, and the iterations threshold has not been reached
# the PLA algorithm continues to iterate
{
cat("Iteration:", k, "\n")
misclassified <- which(sign(y_model) != y)  # getting the indices of the points for which hypothesis is wrong
ifelse (length(misclassified) == 1, n <- misclassified, n <- sample(misclassified, 1))  # randomly choose one of these points
w <- w + y[n] * X[n,]  # update the weights
y_model <- apply(X, 1, function(x) t(w)%*%x)  # use new weights to update the hypothesis function
e_train <- sum(sign(y_model) != y)/length(y_model) # calculate in-sample error
if (e_train < best[[1]]) {
best <- list(e_train, w) # if a the current weight vector is better than the previous best, store it
}
k <- k+1  # increment iteration count
}
e_train <- best[[1]] # updating e_in
w <- best[[2]] # selecting the best weight vector discovered by the algorithm
iterations[i] <- k  # store the number of iterations needed in this run
}
e_in[i] <- e_train   # store the in-sample error
X <- as.matrix(cbind(1, data.generate(N_test)))  # generate test data set to examine out of sample performance
y <- as.numeric(X[, 'x1'] * sim$slope + sim$intercept > X[, 'x2']) * 2 - 1  # classify the test points using the target function generated with the training data set
y_model <- sign(as.vector(X%*%w))  # apply the hypothesis function to the test data
e_out[i] <- sum(y != y_model)/N_test  # calculate and store out-of-sample error
}
library(ggplot2)
library(gridExtra)
plot1 <- qplot(sim$data$x1, sim$data$x2, col= as.factor(sim$data$y), data = sim$data, xlab = 'x1', ylab = 'x2', main = 'Training Data') +
geom_abline(intercept = sim$intercept, slope = sim$slope) +
geom_abline(intercept = -w[1]/w[3], slope = -w[2]/w[3], col=3)
test <- data.frame(x1 = X[, 'x1'], x2 = X[, 'x2'], y = y)
plot2 <- qplot(test$x1, test$x2, col= as.factor(test$y), data = test, xlab = 'x1', ylab = 'x2', main = 'The Same Model Target Function Against Test Data') +
geom_abline(intercept = sim$intercept, slope = sim$slope) +
geom_abline(intercept = -w[1]/w[3], slope = -w[2]/w[3], col=3)
grid.arrange(plot1, plot2, ncol=2)
list(e_in = mean(e_in), e_out = mean(e_out), iterations = mean(iterations)) # return the averages of the error measures and the number of iterations run by the PLA, if applicable
}
simulate.classificationByRegression()
simulate.classificationByRegression(N_train = 10, PLA = TRUE)
simulate.classificationByRegression(N_train = 10, PLA = TRUE)
simulate.classificationByRegression(N_train = 100, PLA = TRUE)
## Function for generating the data and, if specified, a target function
data.generate <- function(n = 10, ext = 1, generateTarget = FALSE){
# Generate the points
x1 <- runif(n, -ext, ext)
x2 <- runif(n, -ext, ext)
if (!generateTarget)
return(data.frame(x1, x2))
# Draw a random line in the area (target function)
point <- runif(2, -ext, ext)
point2 <- runif(2, -ext, ext)
slope <- (point2[2] - point[2]) / (point2[1] - point[1])
intercept <- point[2] - slope * point[1]
# Set up a factor for point classification
y <- as.numeric(x1 * slope + intercept > x2) * 2 - 1
# Return the values in a list
data <- data.frame(x1,x2,y)
return(list(data = data,slope = slope, intercept = intercept))
}
##### PLA  #####
simulate.PLA <- function(numTrials = 1000, maxIterations = Inf, simulation = data.generate) {
iterations <- numeric(0)    # initializing the iteration and misclassification probability vectors
probability <- numeric(0)
# Running the algorithm through numTrials trials
#numTrials <- 1000
for (i in 1:numTrials){
generated  <-  simulation(n=100, generateTarget = TRUE) # generating points (set n=10 or n=100) and target function
input  <-  as.matrix(cbind(1, generated$data[c(1,2)])) # creating the input matrix
w  <-  c(0,0,0)  # initializing the weight vector
#res <- apply(input,1,function(x) t(w)%*%x)  # multiplying transpose of w with each row of input matrix to get initial hypothesis function
res <- as.vector(input %*% w) #equivalent operation
best <- list(sum(sign(res) != generated$data$y)/length(res), w)  # initializing list to keep track of the best in-sample error achieved so far and the weight vector that produced it
k <- 0  # initializing iteration counter
while (any(sign(res) != generated$data$y) && k < maxIterations) # as long as any of the elements of res do not match the true output, y, and the iterations threshold has not been reached
# the PLA algorithm continues to iterate
{
cat("Iteration:", k, "\n")
misclassified <- which(sign(res)!=generated$data$y)  # getting the indices of the points for which hypothesis is wrong
ifelse (length(misclassified)==1, n <- misclassified, n <- sample(misclassified,1))  # randomly choose one of these points
w <- w + generated$data$y[n]*input[n,]  # update the weights
res <- apply(input,1,function(x) t(w)%*%x)  # use new weights to update the hypothesis function
e_in <- sum(sign(res) != generated$data$y)/length(res) # calculate in-sample error
if (e_in < best[[1]]) {
best <- list(e_in, w) # if a the current weight vector is better than the previous best, store it
}
k <- k+1  # increment iteration count
}
w <- best[[2]] #selecting the best weight vector discovered by the algorithm
iterations[i] <- k  # store the number of iterations needed in this run
new.data <- simulation(10000)  #  generating the test points in order to examine out-of-sample performance
f  <-  as.numeric(new.data$x1 * generated$slope + generated$intercept > new.data$x2) * 2 - 1  # classifying points according to the true function f
g  <-  as.numeric(new.data$x1 * (-w[2]/w[3]) - w[1]/w[3] > new.data$x2) * 2 - 1  # classifying points according to the hypothesised function g, using the
# final weights provided by PLA
probability[i] <- sum(f != g)/10000  # store the misclassification error from this run
}
# Plot the points and f and g functions from the last iteration (purely illustrative purposes)
library(ggplot2)
qplot(x1,x2,col= as.factor(y), data = generated$data) + geom_abline(intercept = generated$intercept, slope = generated$slope)+
geom_abline(intercept = -w[1]/w[3], slope = -w[2]/w[3], col=3)
# Final results: average of iterations and estimated misclassification probabilities
list(iterations = mean(iterations), probability = mean(probability))
}
simulate.PLA()
simulate.PLA()
install.packages('devtools')
library(devtools)
find_rtools()
find_rtools()
?c
x <- c(12,43)
p <- c(T, F)
p
p[F]
p[FALSE]
p[2]
p[!p]
p[p]
p[!p] <- x
c(FALSE, 2)
as.POSIXlt(Sys.time())
manes(as.POSIXlt(Sys.time()))
names(as.POSIXlt(Sys.time()))
names(unclass(as.POSIXlt(Sys.time())))
names(unclass(as.POSIXlt(Sys.time())))
as.POSIXlt(Sys.time())$sex
as.POSIXlt(Sys.time())$sec
attributes(as.POSIXlt(Sys.time()))
as.POSIXlt(Sys.time())$names
as.POSIXlt(Sys.time())$gmtoff
?strptime
cube <- function(x, n) {
x^3
}
cube(3)
x <- 1:10
if(x > 5) {
x <- 0
}
x
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
z <- 10
f(3)
x <- 5
y <- if(x < 3) {
NA
} else {
10
}
y
setwd("F:/Self-Education/John Hopkins Data Science Specialization/2. R Programming/week2/programming assignment 1")
list.files()
list.files('specdata/')
length(list.files('specdata/'))
read.csv('specdata//001.csv')
head(read.csv('specdata//001.csv'))
pollutantmean <- function(directory, pollutant, id = 1:332) {
## 'directory' is a character vector of length 1 indicating
## the location of the CSV files
## 'pollutant' is a character vector of length 1 indicating
## the name of the pollutant for which we will calculate the
## mean; either "sulfate" or "nitrate".
## 'id' is an integer vector indicating the monitor ID numbers
## to be used
## Return the mean of the pollutant across all monitors listed
## in the 'id' vector (ignoring NA values)
x <- c()
for(i in id) {
filename <- paste(sprintf('%.03d', i), '.csv', sep = '')
path <- paste(directory, '/', filename, sep = '')
x <- c(x, read.csv(path)[[pollutant]])  #using [[ instead of $ because the index is computed; $ needs a literal name
}
mean(x, na.rm = TRUE)
}
pollutantmean('specdata/', 'nitrate')
pollutantmean('specdata', 'nitrate')
sprintf('%.03d', 332)
sprintf('%.03d', 3)
source("http://d396qusza40orc.cloudfront.net/rprog%2Fscripts%2Fsubmitscript1.R")
submit()
submit()
submit()
submit()
x <- c()
class(x)
y <- NULL
x
y
c(x, 1)
c(y, 1)
c(y, c(1,2)
)
x
class(c(x,1))
pollutantmean('specdata', 'nitrate')
complete("specdata", 30:25)
complete <- function(directory, id = 1:332) {
## 'directory' is a character vector of length 1 indicating
## the location of the CSV files
## 'id' is an integer vector indicating the monitor ID numbers
## to be used
## Return a data frame of the form:
## id nobs
## 1  117
## 2  1041
## ...
## where 'id' is the monitor ID number and 'nobs' is the
## number of complete cases
completeCases <- NULL
for(i in id) {
filename <- paste(sprintf('%03d', i), '.csv', sep = '')
path <- paste(directory, '/', filename, sep = '')
completeCases <- c(completeCases, sum(complete.cases(read.csv(path))))
}
data.frame(id = id, nobs = completeCases)
}
complete("specdata", 30:25)
complete <- function(directory, id = 1:332) {
## 'directory' is a character vector of length 1 indicating
## the location of the CSV files
## 'id' is an integer vector indicating the monitor ID numbers
## to be used
## Return a data frame of the form:
## id nobs
## 1  117
## 2  1041
## ...
## where 'id' is the monitor ID number and 'nobs' is the
## number of complete cases
completeCases <- numeric(length(list.files(directory)))
for(i in id) {
filename <- paste(sprintf('%03d', i), '.csv', sep = '')
path <- paste(directory, '/', filename, sep = '')
completeCases <- c(completeCases, sum(complete.cases(read.csv(path))))
}
data.frame(id = id, nobs = completeCases)
}
complete("specdata", 30:25)
complete <- function(directory, id = 1:332) {
## 'directory' is a character vector of length 1 indicating
## the location of the CSV files
## 'id' is an integer vector indicating the monitor ID numbers
## to be used
## Return a data frame of the form:
## id nobs
## 1  117
## 2  1041
## ...
## where 'id' is the monitor ID number and 'nobs' is the
## number of complete cases
completeCases <- numeric(length(id))
for(i in id) {
filename <- paste(sprintf('%03d', i), '.csv', sep = '')
path <- paste(directory, '/', filename, sep = '')
completeCases <- c(completeCases, sum(complete.cases(read.csv(path))))
}
data.frame(id = id, nobs = completeCases)
}
complete("specdata", 30:25)
complete <- function(directory, id = 1:332) {
## 'directory' is a character vector of length 1 indicating
## the location of the CSV files
## 'id' is an integer vector indicating the monitor ID numbers
## to be used
## Return a data frame of the form:
## id nobs
## 1  117
## 2  1041
## ...
## where 'id' is the monitor ID number and 'nobs' is the
## number of complete cases
completeCases <- NULL
for(i in id) {
filename <- paste(sprintf('%03d', i), '.csv', sep = '')
path <- paste(directory, '/', filename, sep = '')
completeCases <- c(completeCases, sum(complete.cases(read.csv(path))))
}
data.frame(id = id, nobs = completeCases)
}
complete("specdata", 30:25)
complete <- function(directory, id = 1:332) {
## 'directory' is a character vector of length 1 indicating
## the location of the CSV files
## 'id' is an integer vector indicating the monitor ID numbers
## to be used
## Return a data frame of the form:
## id nobs
## 1  117
## 2  1041
## ...
## where 'id' is the monitor ID number and 'nobs' is the
## number of complete cases
completeCases <- numeric(length(id))
for(i in 1:length(id)) {
filename <- paste(sprintf('%03d', id[i]), '.csv', sep = '')
path <- paste(directory, '/', filename, sep = '')
completeCases[i] <- c(completeCases, sum(complete.cases(read.csv(path))))
}
data.frame(id = id, nobs = completeCases)
}
complete("specdata", 30:25)
complete <- function(directory, id = 1:332) {
## 'directory' is a character vector of length 1 indicating
## the location of the CSV files
## 'id' is an integer vector indicating the monitor ID numbers
## to be used
## Return a data frame of the form:
## id nobs
## 1  117
## 2  1041
## ...
## where 'id' is the monitor ID number and 'nobs' is the
## number of complete cases
completeCases <- numeric(length(id))
for(i in 1:length(id)) {
filename <- paste(sprintf('%03d', id[i]), '.csv', sep = '')
path <- paste(directory, '/', filename, sep = '')
completeCases[i] <- sum(complete.cases(read.csv(path)))
}
data.frame(id = id, nobs = completeCases)
}
complete("specdata", 30:25)
submit()
submit()
submit()
submit()
submit()
submit()
?dbconnect
R.homr()
R.home()
